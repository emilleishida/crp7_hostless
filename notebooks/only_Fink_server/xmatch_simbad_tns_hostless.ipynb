{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-3/python/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/opt/spark-3/python/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf\n",
    "from pyspark.sql.types import IntegerType, BinaryType, ArrayType\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fink_filters.classification import extract_fink_classification\n",
    "from fink_utils.spark.utils import concat_col\n",
    "from fink_science.xmatch.utils import cross_match_astropy\n",
    "from fink_science.xmatch.processor import crossmatch_mangrove, crossmatch_other_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/astrolabsoftware/fink-science-portal/blob/b7326ed4febe0e106c1e93565ea622c1c95218e8/assets/spark_ztf_transfer.py#L112C1-L202C14\n",
    "# on 19 FEB 2024\n",
    "def add_classification(spark, df, path_to_tns):\n",
    "    \"\"\" Add classification from Fink & TNS\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark:\n",
    "    df: DataFrame\n",
    "        Spark DataFrame containing ZTF alert data\n",
    "    path_to_tns: str\n",
    "        Path to TNS data (parquet)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input DataFrame with 2 new columns `finkclass` and\n",
    "        `tnsclass` containing classification tags.\n",
    "    \"\"\"\n",
    "    # extract Fink classification\n",
    "    df = df.withColumn(\n",
    "        'finkclass',\n",
    "        extract_fink_classification(\n",
    "            df['cdsxmatch'],\n",
    "            df['roid'],\n",
    "            df['mulens'],\n",
    "            df['snn_snia_vs_nonia'],\n",
    "            df['snn_sn_vs_all'],\n",
    "            df['rf_snia_vs_nonia'],\n",
    "            df['candidate.ndethist'],\n",
    "            df['candidate.drb'],\n",
    "            df['candidate.classtar'],\n",
    "            df['candidate.jd'],\n",
    "            df['candidate.jdstarthist'],\n",
    "            df['rf_kn_vs_nonkn'],\n",
    "            df['tracklet']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pdf_tns_filt = pd.read_parquet(path_to_tns)\n",
    "    pdf_tns_filt_b = spark.sparkContext.broadcast(pdf_tns_filt)\n",
    "\n",
    "    @pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "    def crossmatch_with_tns(objectid, ra, dec):\n",
    "        # TNS\n",
    "        pdf = pdf_tns_filt_b.value\n",
    "        ra2, dec2, type2 = pdf['ra'], pdf['declination'], pdf['type']\n",
    "\n",
    "        # create catalogs\n",
    "        catalog_ztf = SkyCoord(\n",
    "            ra=np.array(ra, dtype=np.float) * u.degree,\n",
    "            dec=np.array(dec, dtype=np.float) * u.degree\n",
    "        )\n",
    "        catalog_tns = SkyCoord(\n",
    "            ra=np.array(ra2, dtype=np.float) * u.degree,\n",
    "            dec=np.array(dec2, dtype=np.float) * u.degree\n",
    "        )\n",
    "\n",
    "        # cross-match\n",
    "        idx, d2d, d3d = catalog_tns.match_to_catalog_sky(catalog_ztf)\n",
    "\n",
    "        sub_pdf = pd.DataFrame({\n",
    "            'objectId': objectid.values,\n",
    "            'ra': ra.values,\n",
    "            'dec': dec.values,\n",
    "        })\n",
    "\n",
    "        # cross-match\n",
    "        idx2, d2d2, d3d2 = catalog_ztf.match_to_catalog_sky(catalog_tns)\n",
    "\n",
    "        # set separation length\n",
    "        sep_constraint2 = d2d2.degree < 1.5 / 3600\n",
    "\n",
    "        sub_pdf['TNS'] = ['Unknown'] * len(sub_pdf)\n",
    "        sub_pdf['TNS'][sep_constraint2] = type2.values[idx2[sep_constraint2]]\n",
    "\n",
    "        to_return = objectid.apply(\n",
    "            lambda x: 'Unknown' if x not in sub_pdf['objectId'].values\n",
    "            else sub_pdf['TNS'][sub_pdf['objectId'] == x].values[0]\n",
    "        )\n",
    "\n",
    "        return to_return\n",
    "\n",
    "    df = df.withColumn(\n",
    "        'v:tns_classification',\n",
    "        crossmatch_with_tns(\n",
    "            df['objectId'],\n",
    "            df['candidate.ra'],\n",
    "            df['candidate.dec']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols0 = ['objectId']\n",
    "cols = [\n",
    "    F.col('cutoutScience.stampData').alias('b:cutoutScience_stampData'),\n",
    "    F.col('cutoutTemplate.stampData').alias('b:cutoutTemplate_stampData'),\n",
    "    F.col('cutoutDifference.stampData').alias('b:cutoutDifference_stampData'),\n",
    "    F.col('candidate.aimage').alias('i:aimage'),\n",
    "    F.col('candidate.aimagerat').alias('i:aimagerat'),\n",
    "    F.col('candidate.bimage').alias('i:bimage'),\n",
    "    F.col('candidate.bimagerat').alias('i:bimagerat'),\n",
    "    F.col('candidate.candid').alias('i:candid'),\n",
    "    F.col('candidate.chinr').alias('i:chinr'),\n",
    "    F.col('candidate.chipsf').alias('i:chipsf'),\n",
    "    F.col('candidate.classtar').alias('i:classtar'),\n",
    "    F.col('candidate.dec').alias('i:dec'),\n",
    "    F.col('candidate.fid').alias('i:fid'),\n",
    "    F.col('candidate.fwhm').alias('i:fwhm'),\n",
    "    F.col('candidate.isdiffpos').alias('i:isdiffpos'),\n",
    "    F.col('candidate.jd').alias('i:jd'),\n",
    "    F.col('candidate.maggaia').alias('i:maggaia'),\n",
    "    F.col('candidate.maggaiabright').alias('i:maggaiabright'),\n",
    "    F.col('candidate.magpsf').alias('i:magpsf'),\n",
    "    F.col('candidate.neargaia').alias('i:neargaia'),\n",
    "    F.col('candidate.neargaiabright').alias('i:neargaiabright'),\n",
    "    F.col('candidate.ra').alias('i:ra'),\n",
    "    F.col('candidate.sigmapsf').alias('i:sigmapsf'),\n",
    "    F.col('cdsxmatch'),\n",
    "    F.col('roid'),\n",
    "    F.col('mulens'),\n",
    "    F.col('snn_snia_vs_nonia'),\n",
    "    F.col('snn_sn_vs_all'),\n",
    "    F.col('rf_snia_vs_nonia'),\n",
    "    F.col('candidate.ndethist'),\n",
    "    F.col('candidate.drb'),\n",
    "    F.col('candidate.classtar'),\n",
    "    F.col('candidate.jd'),\n",
    "    F.col('candidate.jdstarthist'),\n",
    "    F.col('rf_kn_vs_nonkn'),\n",
    "    F.col('tracklet'),\n",
    "    #F.col('mangrove'),\n",
    "    F.col('finkclass').alias('v:classification'),\n",
    "    F.col('v:tns_classification')\n",
    "]\n",
    "\n",
    "epochs = {\n",
    "    'epoch1': [\n",
    "        '../julien.peloton/archive/science/year=2019',\n",
    "        '../julien.peloton/archive/science/year=2020',\n",
    "        '../julien.peloton/archive/science/year=2021',\n",
    "    ],\n",
    "    'epoch2': [\n",
    "        '../julien.peloton/archive/science/year=2022',\n",
    "        '../julien.peloton/archive/science/year=2023',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# path to TNS data\n",
    "path_to_tns = '/spark_mongo_tmp/julien.peloton/tns.parquet'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Available TNS classes\n",
    "[Row(v:tns_classification='(TNS) Other'),\n",
    " Row(v:tns_classification='(TNS) SN Ia'),\n",
    " Row(v:tns_classification='Unknown'),\n",
    " Row(v:tns_classification='(TNS) SN II'),\n",
    " Row(v:tns_classification='(TNS) AGN'),\n",
    " Row(v:tns_classification='(TNS) SLSN-I'),\n",
    " Row(v:tns_classification='(TNS) CV'),\n",
    " Row(v:tns_classification='(TNS) SN Ia-91bg-like'),\n",
    " Row(v:tns_classification='(TNS) SN IIn'),\n",
    " Row(v:tns_classification='(TNS) SLSN-II'),\n",
    " Row(v:tns_classification='(TNS) SN Iax[02cx-like]'),\n",
    " Row(v:tns_classification='(TNS) Nova'),\n",
    " Row(v:tns_classification='(TNS) SN I'),\n",
    " Row(v:tns_classification='(TNS) SN Ib'),\n",
    " Row(v:tns_classification='(TNS) SN Ia-91T-like'),\n",
    " Row(v:tns_classification='(TNS) SN IIb'),\n",
    " Row(v:tns_classification='(TNS) FRB'),\n",
    " Row(v:tns_classification='(TNS) SN IIP'),\n",
    " Row(v:tns_classification='(TNS) SN Ic'),\n",
    " Row(v:tns_classification='(TNS) SN Ia-CSM'),\n",
    " Row(v:tns_classification='(TNS) SN Ic-BL'),\n",
    " Row(v:tns_classification='(TNS) TDE'),\n",
    " Row(v:tns_classification='(TNS) Varstar'),\n",
    " Row(v:tns_classification='(TNS) QSO'),\n",
    " Row(v:tns_classification='(TNS) SN Ia-pec'),\n",
    " Row(v:tns_classification='(TNS) Gap'),\n",
    " Row(v:tns_classification='(TNS) SN'),\n",
    " Row(v:tns_classification='(TNS) SN Ibn'),\n",
    " Row(v:tns_classification='(TNS) M dwarf'),\n",
    " Row(v:tns_classification='(TNS) ILRT'),\n",
    " Row(v:tns_classification='(TNS) SN II-pec'),\n",
    " Row(v:tns_classification='(TNS) SN Icn'),\n",
    " Row(v:tns_classification='(TNS) SN Ib-pec'),\n",
    " Row(v:tns_classification='(TNS) SN Ib/c')]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classes not wanted in TNS\n",
    "not_wanted_tns = ['(TNS) CV', '(TNS) Varstar', '(TNS) M dwarf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extragalactic_uniqclass_best = ['AGN',\n",
    "                                'AGN_Candidate',\n",
    "                                'SN',\n",
    "                                'BH_Candidate', \n",
    "                                'Blazar', \n",
    "                                'Blazar_Candidate',\n",
    "                                'BLLac', \n",
    "                                'BLLac_Candidate', \n",
    "                                'Candidate_Nova', \n",
    "                                'Candidate_NS',\n",
    "                                'Candidate_SN*', \n",
    "                                'LINER',\n",
    "                                'QSO',\n",
    "                                'QSO_Candidate'\n",
    "                                'Seyfert', \n",
    "                                'Seyfert1', \n",
    "                                'Seyfert2', \n",
    "                                'Seyfert_1',\n",
    "                                'Seyfert_2',\n",
    "                                'SN', \n",
    "                                'SN*_Candidate',\n",
    "                                'Supernova', \n",
    "                                'ULX', \n",
    "                                'ULX?', \n",
    "                                'ULX_Candidate',\n",
    "                                ###### until here the list came from Priscila\n",
    "                                'SN candidate',             # from SuperNNova\n",
    "                                'Early SN Ia candidate',    # from AL + random forest\n",
    "                                'Kilonova candidate'       # from PCA + random forest\n",
    "                                ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark functions created by Emille\n",
    "\n",
    "# function that identifies classes different from Unknown\n",
    "filter_known = udf(lambda x: x != 'Unknown', BooleanType())\n",
    "\n",
    "# function that check if a given entry is within  the required Fink (ML + SIMBAD) classes\n",
    "filter_classes = udf(lambda x: x in extragalactic_uniqclass_best, BooleanType())\n",
    "\n",
    "# function that checks if an object exists in hyperLEDA\n",
    "filter_hyperLEDA = udf(lambda x: x['HyperLEDA_name'] == 'None', BooleanType())\n",
    "\n",
    "# function that checks if an object exists in 2MASS\n",
    "filter_2MASS = udf(lambda x: x['2MASS_name'] == 'None', BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/20 05:23:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# flag to pring intermediate steps. This take a long time! use with caution\n",
    "debug = False\n",
    "\n",
    "# read data\n",
    "\n",
    "# read one month for quick testing\n",
    "#df1 = spark.read.format('parquet').load('../julien.peloton/archive/science/year=2023/month=04/')\n",
    "\n",
    "# read entire first epoch\n",
    "df1 = spark.read.format('parquet').option('basePath', '../julien.peloton/archive/science').load(epochs['epoch2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    tot = df1.count()\n",
    "    print('Just read: ', tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross match with TNS\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df1_with_classes = add_classification(spark, df=df1, path_to_tns=path_to_tns)\n",
    "\n",
    "# get the columns we want\n",
    "df1_renamed = df1_with_classes.select(cols0 + cols)\n",
    "\n",
    "# cross match with Mangrove\n",
    "df1_with_classes = df1_renamed.withColumn('mangrove',\n",
    "                      crossmatch_mangrove(df1_renamed['i:candid'], df1_renamed['i:ra'], \n",
    "                                          df1_renamed['i:dec'], F.lit(60.0)))\n",
    "\n",
    "# cross match with GCVS\n",
    "df1_with_classes = df1_with_classes.withColumn('gcvs', \n",
    "                                             crossmatch_other_catalog(F.col('i:candid'), \n",
    "                                                                      F.col('i:ra'), \n",
    "                                                                      F.col('i:dec'),\n",
    "                                                                      F.lit('gcvs')))\n",
    "\n",
    "# cross match with VSX\n",
    "df1_with_classes = df1_with_classes.withColumn('vsx', \n",
    "                                             crossmatch_other_catalog(F.col('i:candid'), \n",
    "                                                                      F.col('i:ra'), \n",
    "                                                                      F.col('i:dec'),\n",
    "                                                                      F.lit('vsx')))\n",
    "\n",
    "# cross match with 3hsp\n",
    "df1_with_classes = df1_with_classes.withColumn('3hsp', \n",
    "                                             crossmatch_other_catalog(F.col('i:candid'), \n",
    "                                                                      F.col('i:ra'), \n",
    "                                                                      F.col('i:dec'),\n",
    "                                                                      F.lit('3hsp')))\n",
    "\n",
    "# cross match with VSX\n",
    "df1_with_classes = df1_with_classes.withColumn('4lac', \n",
    "                                             crossmatch_other_catalog(F.col('i:candid'), \n",
    "                                                                      F.col('i:ra'), \n",
    "                                                                      F.col('i:dec'),\n",
    "                                                                      F.lit('4lac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_with_classes.select('v:classification', 'v:tns_classification', 'gcvs', 'vsx', '3hsp', '4lac','spicy').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove things that do not have a counter part in the fink ML classes nor SIMBAD nor TNS\n",
    "df1_known = df1_with_classes.filter(filter_known(F.col('v:tns_classification')) | \\\n",
    "                               filter_known(F.col('v:classification')))\n",
    "\n",
    "if debug:\n",
    "    # count number of alerts\n",
    "    tot_df1_known = df1_known.count()\n",
    "\n",
    "    print('After unknown in both TNS and Fink classes: ', tot_df1_known, tot_df1_known/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_known.select('v:tns_classification', 'v:classification').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unwanted classes in TNS\n",
    "df1_filtered_tns = df1_known.filter(~F.col('v:tns_classification').isin(not_wanted_tns))\n",
    "\n",
    "if debug:\n",
    "    \n",
    "    # get number of surviving alerts\n",
    "    tot_filtered_tns = df1_filtered_tns.count()\n",
    "\n",
    "    print('After unwanted TNS classes: ', tot_filtered_tns, tot_filtered_tns/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_filtered_tns.select('v:tns_classification').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only objects in the required Fink (ML), SIMBAD or TNS required classes\n",
    "df1_filtered_classes = df1_filtered_tns.filter(filter_classes(F.col('v:classification'))| \\\n",
    "                                               filter_known(F.col('v:tns_classification')))\n",
    "\n",
    "if debug:\n",
    "    # get number of surviving alerts\n",
    "    tot_df1_filtered_classes = df1_filtered_classes.count()\n",
    "\n",
    "    print('After filtering classes: ', tot_df1_filtered_classes, tot_df1_filtered_classes/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_filtered_classes.select('v:classification', 'v:tns_classification').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter objects that do not have a counterpart in Mangrove\n",
    "df1_filtered_final = df1_filtered_classes.filter(filter_hyperLEDA(F.col(\"mangrove\")) &  \\\n",
    "                                                filter_2MASS(F.col(\"mangrove\")))\n",
    "\n",
    "if debug:\n",
    "    # get number of alerts without a mangrove host \n",
    "    tot_df1_filtered_final = df1_filtered_final.count()\n",
    "\n",
    "    print('After filtering mangrove: ', tot_df1_filtered_final, tot_df1_filtered_final/tot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_filtered_final.select('mangrove').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which columns to keep\n",
    "cols_ = [i for i in df1_filtered_final.columns if i != 'objectId']\n",
    "df1_output = df1_filtered_final.select(cols0 + cols_)\n",
    "\n",
    "# aggregate output by objectId\n",
    "df1_agg = df1_output.groupBy('objectId')\\\n",
    "    .agg(*[F.collect_list(col).alias(col) for col in cols_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    df1_agg.select('v:classification').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # count number of unique objects\n",
    "    tot_obj = df1_agg.count()\n",
    "\n",
    "    print(tot_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======>                                           (3852 + 20) / 25306]\r"
     ]
    }
   ],
   "source": [
    "# write result to file\n",
    "df1_agg.write.mode('overwrite').parquet('SIMBAD_not_in_MANGROVE_with_candidates_2022_2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the size of the output\n",
    "! hdfs dfs -du -h | grep SIMBAD_not_in_MANGROVE_with_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# count number of alerts\n",
    "tot_df1_known = df1_known.count()\n",
    "\n",
    "print('After unknown in both TNS and Fink classes: ', tot_df1_known, tot_df1_known/tot)\n",
    "\n",
    "# get number of surviving alerts\n",
    "tot_filtered_tns = df1_filtered_tns.count()\n",
    "\n",
    "print('After unwanted TNS classes: ', tot_filtered_tns, tot_filtered_tns/tot)\n",
    "\n",
    "# get number of surviving alerts\n",
    "tot_df1_filtered_classes = df1_filtered_classes.count()\n",
    "\n",
    "print('After filtering classes: ', tot_df1_filtered_classes, tot_df1_filtered_classes/tot)\n",
    "\n",
    "tot_df1_filtered_final = df1_filtered_final.count()\n",
    "\n",
    "print('After filtering mangrove: ', tot_df1_filtered_final, tot_df1_filtered_final/tot) \n",
    "\n",
    "# count number of unique objects\n",
    "tot_obj = df1_agg.count()\n",
    "\n",
    "print('Number of surviving objects:', tot_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
